{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5ForConditionalGeneration, T5Model\n",
    "from transformers import T5Config\n",
    "\n",
    "from tokenization_enc_dec import EncDecTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EncDecTokenizer('./EVA/src/bpe_dialog_new/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp_rank_00_model_states.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../eva-ckpt/222500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../eva-ckpt/222500/mp_rank_00_model_states.pt', map_location='cpu')['module']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeds.weight torch.Size([30000, 2048])\n",
      "lm_head.weight torch.Size([30000, 2048])\n",
      "encoder.word_embeds.weight torch.Size([30000, 2048])\n",
      "encoder.final_layernorm.weight torch.Size([2048])\n",
      "encoder.blocks.0.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 32])\n",
      "encoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.0.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.0.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.1.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.1.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.1.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.2.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.2.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.2.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.3.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.3.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.3.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.4.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.4.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.4.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.5.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.5.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.5.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.6.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.6.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.6.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.7.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.7.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.7.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.8.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.8.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.8.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.9.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.9.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.9.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.10.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.10.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.10.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.11.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.11.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.11.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.12.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.12.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.12.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.13.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.13.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.13.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.14.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.14.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.14.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.15.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.15.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.15.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.16.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.16.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.16.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.17.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.17.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.17.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.18.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.18.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.18.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.19.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.19.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.19.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.20.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.20.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.20.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.21.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.21.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.21.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.22.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.22.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.22.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.23.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.23.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.23.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.word_embeds.weight torch.Size([30000, 2048])\n",
      "decoder.final_layernorm.weight torch.Size([2048])\n",
      "decoder.blocks.0.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 32])\n",
      "decoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.0.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.0.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.0.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.0.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.1.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.1.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.1.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.1.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.1.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.2.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.2.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.2.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.2.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.2.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.3.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.3.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.3.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.3.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.3.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.4.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.4.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.4.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.4.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.4.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.5.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.5.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.5.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.5.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.5.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.6.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.6.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.6.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.6.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.6.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.7.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.7.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.7.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.7.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.7.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.8.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.8.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.8.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.8.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.8.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.9.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.9.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.9.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.9.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.9.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.10.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.10.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.10.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.10.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.10.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.11.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.11.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.11.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.11.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.11.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.12.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.12.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.12.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.12.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.12.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.13.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.13.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.13.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.13.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.13.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.14.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.14.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.14.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.14.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.14.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.15.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.15.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.15.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.15.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.15.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.16.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.16.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.16.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.16.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.16.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.17.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.17.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.17.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.17.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.17.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.18.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.18.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.18.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.18.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.18.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.19.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.19.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.19.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.19.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.19.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.20.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.20.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.20.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.20.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.20.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.21.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.21.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.21.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.21.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.21.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.22.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.22.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.22.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.22.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.22.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.23.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.23.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.23.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.23.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.23.ff.layer_norm.weight torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# 注意encoder和decoder的blocks.0和其他的1,2,3...23不一样，0多了relative_attention_bias\n",
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(name):\n",
    "    return state_dict[name].numpy()\n",
    "\n",
    "encoder_names0 = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names0 = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "encoder_names = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "def get_block_weight(n, t='encoder', dim=2048):\n",
    "    weights = []\n",
    "    for k, v in state_dict.items():\n",
    "        if t in k and f'blocks.{n}.' in k:\n",
    "            # pytorch和tensorflow版本的weights是矩阵转置的\n",
    "            w = v.numpy()\n",
    "            if 'self_attn.project' in k:\n",
    "                w0, w1, w2 = w[:dim, :], w[dim:dim*2, :], w[dim*2:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "#                 w2 = np.transpose(w2)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "                weights.append((k, w2))\n",
    "            elif 'cross_attn.project_q' in k:\n",
    "#                 w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "            elif 'cross_attn.project_kv' in k:\n",
    "                w0, w1 = w[:dim, :], w[dim:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "            else:\n",
    "#                 if 'dense' in k:\n",
    "#                     w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "    if 'relative_attention_bias' in weights[3][0]:\n",
    "        weights[3], weights[4] = weights[4], weights[3]\n",
    "    weights = [x[1] for x in weights]\n",
    "    if 'encoder' == t:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(encoder_names0 if n == 0 else encoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    else:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(decoder_names0 if n == 0 else decoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config(\n",
    "    vocab_size=30000,\n",
    "    # n_positions=self.n_positions,\n",
    "    d_model=2048,\n",
    "    d_ff=5120,\n",
    "    d_kv=2048 // 32,\n",
    "    num_layers=24,\n",
    "    num_heads=32,\n",
    "    relative_attention_num_buckets=32,\n",
    "    dropout_rate=0.0,\n",
    "    initializer_factor=1.0,\n",
    "    eos_token_id=tokenizer.eod_id,\n",
    "    bos_token_id=tokenizer.pad_id,\n",
    "    pad_token_id=tokenizer.pad_id,\n",
    "    decoder_start_token_id=tokenizer.pad_id,\n",
    "    feed_forward_proj='gated-gelu',\n",
    "    tie_word_embeddings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model(input_ids=torch.LongTensor([[1]]), decoder_input_ids=torch.LongTensor([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7eff84acb580>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight torch.Size([30000, 2048])\n",
      "encoder.embed_tokens.weight torch.Size([30000, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 32])\n",
      "encoder.block.0.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.0.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.1.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.2.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.3.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.4.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.5.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.6.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.7.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.8.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.9.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.10.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.11.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.12.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.12.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.13.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.13.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.14.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.14.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.15.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.15.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.16.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.16.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.17.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.17.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.18.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.18.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.19.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.19.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.20.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.20.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.21.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.21.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.22.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.22.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.23.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.23.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.final_layer_norm.weight torch.Size([2048])\n",
      "decoder.embed_tokens.weight torch.Size([30000, 2048])\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 32])\n",
      "decoder.block.0.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.0.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.0.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.1.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.1.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.2.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.2.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.3.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.3.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.4.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.4.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.5.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.5.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.6.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.6.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.7.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.7.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.8.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.8.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.8.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.8.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.9.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.9.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.9.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.9.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.10.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.10.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.10.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.10.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.11.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.11.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.11.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.11.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.12.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.12.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.12.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.12.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.12.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.13.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.13.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.13.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.13.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.13.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.14.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.14.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.14.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.14.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.14.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.15.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.15.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.15.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.15.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.15.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.16.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.16.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.16.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.16.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.16.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.17.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.17.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.17.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.17.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.17.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.18.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.18.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.18.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.18.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.18.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.19.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.19.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.19.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.19.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.19.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.20.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.20.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.20.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.20.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.20.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.21.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.21.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.21.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.21.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.21.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.22.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.22.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.22.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.22.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.22.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.23.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.0.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.23.layer.1.EncDecAttention.q.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.1.EncDecAttention.k.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.1.EncDecAttention.v.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.1.EncDecAttention.o.weight torch.Size([2048, 2048])\n",
      "decoder.block.23.layer.1.layer_norm.weight torch.Size([2048])\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.block.23.layer.2.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.block.23.layer.2.layer_norm.weight torch.Size([2048])\n",
      "decoder.final_layer_norm.weight torch.Size([2048])\n",
      "lm_head.weight torch.Size([30000, 2048])\n"
     ]
    }
   ],
   "source": [
    "# transformers的T5是把QKV分开的\n",
    "for k, v in model.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.0.layer.0.SelfAttention.q.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 32)\n",
      "encoder.block.0.layer.0.layer_norm.weight (2048,)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight (5120, 2048)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight (5120, 2048)\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight (2048, 5120)\n",
      "encoder.block.0.layer.1.layer_norm.weight (2048,)\n"
     ]
    }
   ],
   "source": [
    "for x in get_block_weight(0, t='encoder').items():\n",
    "    print(x[0], x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.1.layer.0.SelfAttention.q.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.layer_norm.weight (2048,)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight (5120, 2048)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight (5120, 2048)\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight (2048, 5120)\n",
      "encoder.block.1.layer.1.layer_norm.weight (2048,)\n"
     ]
    }
   ],
   "source": [
    "for x in get_block_weight(1, t='encoder').items():\n",
    "    print(x[0], x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.block.0.layer.0.SelfAttention.q.weight (2048, 2048)\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight (2048, 2048)\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight (2048, 2048)\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight (2048, 2048)\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 32)\n",
      "decoder.block.0.layer.0.layer_norm.weight (2048,)\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight (2048, 2048)\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight (2048, 2048)\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight (2048, 2048)\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight (2048, 2048)\n",
      "decoder.block.0.layer.1.layer_norm.weight (2048,)\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight (5120, 2048)\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight (5120, 2048)\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight (2048, 5120)\n",
      "decoder.block.0.layer.2.layer_norm.weight (2048,)\n"
     ]
    }
   ],
   "source": [
    "for x in get_block_weight(0, t='decoder').items():\n",
    "    print(x[0], x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.block.1.layer.0.SelfAttention.q.weight (2048, 2048)\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight (2048, 2048)\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight (2048, 2048)\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight (2048, 2048)\n",
      "decoder.block.1.layer.0.layer_norm.weight (2048,)\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight (2048, 2048)\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight (2048, 2048)\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight (2048, 2048)\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight (2048, 2048)\n",
      "decoder.block.1.layer.1.layer_norm.weight (2048,)\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight (5120, 2048)\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight (5120, 2048)\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight (2048, 5120)\n",
      "decoder.block.1.layer.2.layer_norm.weight (2048,)\n"
     ]
    }
   ],
   "source": [
    "for x in get_block_weight(1, t='decoder').items():\n",
    "    print(x[0], x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared.weight torch.Size([30000, 2048])\n",
    "# encoder.embed_tokens.weight torch.Size([30000, 2048])\n",
    "model_new_weights = OrderedDict()\n",
    "model_new_weights['shared.weight'] = get_weight('word_embeds.weight')\n",
    "model_new_weights['encoder.embed_tokens.weight'] = get_weight('encoder.word_embeds.weight')\n",
    "for i in range(24):\n",
    "    for k, v in get_block_weight(i, t='encoder').items():\n",
    "        model_new_weights[k] = v\n",
    "\n",
    "model_new_weights['encoder.final_layer_norm.weight'] = get_weight('encoder.final_layernorm.weight')\n",
    "\n",
    "for i in range(24):\n",
    "    for k, v in get_block_weight(i, t='decoder').items():\n",
    "        model_new_weights[k] = v\n",
    "\n",
    "model_new_weights['decoder.final_layer_norm.weight'] = get_weight('decoder.final_layernorm.weight')\n",
    "\n",
    "model_new_weights['decoder.embed_tokens.weight'] = get_weight('decoder.word_embeds.weight')\n",
    "model_new_weights['lm_head.weight'] = get_weight('lm_head.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict({k: torch.from_numpy(v) for k, v in model_new_weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好\n",
      "你好,\n",
      "你好,我\n",
      "你好,我是\n",
      "你好,我是你\n",
      "你好,我是你的\n",
      "你好,我是你的粉丝\n",
      "你好,我是你的粉丝<sep>\n",
      "你好,我是你的粉丝<sep><sep>\n",
      "你好,我是你的粉丝<sep><sep>,\n"
     ]
    }
   ],
   "source": [
    "decoder_input = [tokenizer.get_sentinel_id(0)]\n",
    "input_ids = torch.LongTensor([tokenizer.encode('你好啊') + [tokenizer.sep_id, tokenizer.get_sentinel_id(0)]])\n",
    "\n",
    "for i in range(10):\n",
    "    decoder_input_ids = torch.LongTensor([decoder_input])\n",
    "    outputs = model(input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    next_token = np.argsort(outputs.logits[0, -1, :].detach().numpy())[-1]\n",
    "    decoder_input.append(next_token)\n",
    "    print(tokenizer.decode(decoder_input[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./torch_eva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11G\ttorch_eva\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh torch_eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>你好\n",
      "[5503, 4, 29810]\n",
      "请问您是咨询之前的问题还是有其他的问题需要处理呢?\n",
      ">>>\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "\n",
    "while True:\n",
    "    text = input('>>>')\n",
    "    if text == 'quit' or len(text.strip()) <= 0:\n",
    "        break\n",
    "    sentence.append(text)\n",
    "\n",
    "    input_ids = []\n",
    "    for x in sentence:\n",
    "        input_ids += tokenizer.encode(x) + [tokenizer.sep_id]\n",
    "    input_ids += [tokenizer.get_sentinel_id(0)]\n",
    "    print(input_ids)\n",
    "    input_ids = torch.LongTensor([input_ids])\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        decoder_start_token_id=tokenizer.get_sentinel_id(0),\n",
    "        eos_token_id=tokenizer.sep_id,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50\n",
    "    )\n",
    "    out_text = tokenizer.decode(out.numpy()[0].tolist()[1:-1])\n",
    "    print(out_text)\n",
    "    sentence.append(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '你好'\n",
    "decode_text = '你也好啊'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(input_text) + [tokenizer.sep_id, tokenizer.get_sentinel_id(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = [tokenizer.get_sentinel_id(0)] + tokenizer.encode(decode_text)\n",
    "labels = tokenizer.encode(decode_text) + [tokenizer.sep_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(\n",
    "    torch.LongTensor(input_ids).unsqueeze(0),\n",
    "    decoder_input_ids=torch.LongTensor(decoder_input_ids).unsqueeze(0),\n",
    "    labels=torch.LongTensor(labels).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4332, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
