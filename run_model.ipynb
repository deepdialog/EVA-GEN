{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = torch.load('../eva-ckpt/222500/mp_rank_00_model_states.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weight(model, name):\n",
    "    for n, w in model['module'].items():\n",
    "        if name == n:\n",
    "            return w, list(w.shape)\n",
    "\n",
    "def combine(n, dim=0):\n",
    "    return torch.cat([\n",
    "        find_weight(x, n)[0]\n",
    "        for x in (m0, m1, m2, m3)\n",
    "    ], dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, List\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "\n",
    "from configuration_enc_dec import EncDecConfig\n",
    "from tokenization_enc_dec import EncDecTokenizer\n",
    "from model import TorchEncDecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.571 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# !pip install jieba --user\n",
    "jieba.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = EncDecConfig(\n",
    "    d_model=2048,\n",
    "    d_ff=5120,\n",
    "    d_kv=64,\n",
    "    num_heads=32,\n",
    "    num_layers=24,\n",
    "    num_decoder_layers=24,\n",
    "    dropout_rate=0.0,\n",
    "    feed_forward_proj=\"relu\",\n",
    "    init_method_std=0.001,\n",
    "    initializer_factor=1.0,\n",
    "    layer_norm_epsilon=1e-06,\n",
    "    max_position_embeddings=512,\n",
    "    use_cache=True,\n",
    "    use_scaled_init_for_output_weights=True,\n",
    "    do_dim_trick=False\n",
    ")\n",
    "config.vocab_size = 30000\n",
    "config.vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model\n"
     ]
    }
   ],
   "source": [
    "print('build model')\n",
    "model = TorchEncDecModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To float16 and GPU\n",
    "if torch.cuda.is_available():\n",
    "    model = model.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for k, v in model.state_dict().items():\n",
    "#     print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('load state')\n",
    "model.load_state_dict(m0['module'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to eval\n"
     ]
    }
   ],
   "source": [
    "print('to eval')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EncDecTokenizer('./EVA/src/bpe_dialog_new/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_and_position_ids(tokenizer,\n",
    "                               contexts,\n",
    "                               targets,\n",
    "                               reset_position_ids,\n",
    "                               reset_attention_mask):\n",
    "    # Extract batch size and sequence length.\n",
    "    batch_size, enc_seq_length = contexts.size()\n",
    "\n",
    "    # Enc Attention mask.\n",
    "    enc_attn_mask = torch.zeros(\n",
    "        batch_size, 1, enc_seq_length, enc_seq_length, device=contexts.device)\n",
    "\n",
    "    ctx_lengths = (contexts != tokenizer.pad_id).sum(1)\n",
    "    for b in range(batch_size):\n",
    "        enc_attn_mask[b, 0, :ctx_lengths[b], :ctx_lengths[b]] = 1\n",
    "\n",
    "    # Enc Position ids.\n",
    "    enc_pos_ids = torch.arange(\n",
    "        enc_seq_length, dtype=torch.long, device=contexts.device)\n",
    "    enc_pos_ids = enc_pos_ids.unsqueeze(0).expand_as(contexts)\n",
    "    # We need to clone as the ids will be modifed based on batch index.\n",
    "    if reset_position_ids:\n",
    "        enc_pos_ids = enc_pos_ids.clone()\n",
    "\n",
    "    batch_size, dec_seq_length = targets.size()\n",
    "    # Dec Attention mask\n",
    "    dec_attn_mask = torch.tril(torch.ones(\n",
    "        batch_size, 1, dec_seq_length, dec_seq_length, device=targets.device))\n",
    "\n",
    "    # Dec Position ids.\n",
    "    dec_pos_ids = torch.arange(\n",
    "        dec_seq_length, dtype=torch.long, device=targets.device)\n",
    "    dec_pos_ids = dec_pos_ids.unsqueeze(0).expand_as(targets)\n",
    "    # We need to clone as the ids will be modifed based on batch index.\n",
    "    if reset_position_ids:\n",
    "        dec_pos_ids = dec_pos_ids.clone()\n",
    "\n",
    "    # Cross Attention Mask\n",
    "    cross_attn_mask = torch.zeros(\n",
    "        batch_size, 1, dec_seq_length, enc_seq_length, device=contexts.device)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        cross_attn_mask[b, 0, :, :ctx_lengths[b]] = 1\n",
    "\n",
    "    model_batch = {\n",
    "        \"enc_attention_mask\": enc_attn_mask,\n",
    "        \"enc_position_ids\": enc_pos_ids,\n",
    "        \"dec_attention_mask\": dec_attn_mask,\n",
    "        \"dec_position_ids\": dec_pos_ids,\n",
    "        \"cross_attention_mask\": cross_attn_mask,\n",
    "    }\n",
    "\n",
    "    return model_batch\n",
    "\n",
    "\n",
    "def get_inference_batch(\n",
    "        context_tokens,\n",
    "        device,\n",
    "        batch_size,\n",
    "        target_length,\n",
    "        tokenizer\n",
    "    ):\n",
    "    tokens = context_tokens\n",
    "    tokens = tokens.view(batch_size, -1).contiguous()\n",
    "    tokens = tokens.to(device)\n",
    "    \n",
    "    targets = torch.zeros(batch_size, target_length, dtype=torch.long, device=device) + tokenizer.get_sentinel_id(0)\n",
    "\n",
    "    # Get the masks and postition ids.\n",
    "    model_batch = get_masks_and_position_ids(\n",
    "        tokenizer,\n",
    "        tokens,\n",
    "        targets,\n",
    "        False, # args.reset_position_ids,\n",
    "        False, # args.reset_attention_mask,\n",
    "    )\n",
    "    \n",
    "    model_batch = {\n",
    "        \"enc_input_ids\": tokens,\n",
    "        \"dec_input_ids\": targets,\n",
    "        **model_batch\n",
    "    }\n",
    "\n",
    "    return model_batch\n",
    "\n",
    "\n",
    "def top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-10000, remove_unk=False):\n",
    "    # This function has been mostly taken from huggingface conversational ai code at\n",
    "    # https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n",
    "\n",
    "    if remove_unk:\n",
    "        logits[..., 0] = filter_value\n",
    "\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    batch_size = logits.size()[0]\n",
    "    if top_p > 0.0:\n",
    "        logits=logits.view(batch_size, -1).contiguous()\n",
    "        for logit in logits:\n",
    "            sorted_logits, sorted_indices = torch.sort(logit, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift the indices to the right to keep also the first token above the threshold\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logit[indices_to_remove] = filter_value\n",
    "\n",
    "        logits=logits.view(batch_size, -1).contiguous()\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def calc_banned_ngram_tokens(prev_input_ids, num_hypos: int, no_repeat_ngram_size: int, cur_len: int, vocab_size: int):\n",
    "    generated_ngrams = [{tuple([23]):[33, 31], tuple([31]):[123]} for _ in range(num_hypos)]\n",
    "    def _get_generated_ngrams(hypo_idx):\n",
    "        # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "        start_idx = cur_len + 1 - no_repeat_ngram_size\n",
    "        ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].tolist())\n",
    "        penalty_idx = tuple(prev_input_ids[hypo_idx, cur_len - 1: cur_len].tolist())\n",
    "        return generated_ngrams[hypo_idx].get(ngram_idx, []) + generated_ngrams[hypo_idx].get(penalty_idx, [])\n",
    "\n",
    "    if cur_len + 1 < no_repeat_ngram_size:\n",
    "        if cur_len > 0:\n",
    "            return [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n",
    "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "    #generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx].tolist()\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(no_repeat_ngram_size)]):\n",
    "            if any(e >= vocab_size for e in ngram):\n",
    "                continue\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "\n",
    "    banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n",
    "    return banned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(\n",
    "    model, tokenizer, sents,\n",
    "    device='cpu',\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    no_repeat_ngram_size = 3,\n",
    "    repetition_penalty = 1.2\n",
    "):\n",
    "    batch_size = 1\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        all_input_tokens = []\n",
    "        for sent in sents:\n",
    "            all_input_tokens.extend(tokenizer.encode(sent) + [tokenizer.sep_id])\n",
    "        all_input_tokens.extend([tokenizer.get_sentinel_id(0)])\n",
    "\n",
    "        input_len = len(all_input_tokens)\n",
    "        length_tensor = torch.tensor([input_len], dtype=torch.long).to(device)\n",
    "        token_tensor = torch.tensor(all_input_tokens, dtype=torch.long).to(device)\n",
    "        token_tensor = token_tensor.unsqueeze(0)\n",
    "\n",
    "        target_length = max_length\n",
    "\n",
    "        model_batch = get_inference_batch(token_tensor, device, batch_size, target_length, tokenizer)\n",
    "\n",
    "        enc_input_ids = model_batch['enc_input_ids']\n",
    "        enc_attention_mask = model_batch['enc_attention_mask']\n",
    "        enc_position_ids = model_batch['enc_position_ids']\n",
    "\n",
    "        enc_outputs = model(\n",
    "            enc_input_ids=enc_input_ids,\n",
    "            only_encoder=True,\n",
    "            enc_attention_mask=enc_attention_mask,\n",
    "            enc_position_ids=enc_position_ids\n",
    "        )\n",
    "        enc_hidden_states = enc_outputs[\"encoder_last_hidden_state\"]\n",
    "\n",
    "        # for generating responses\n",
    "        # we only use the <go> token, so truncate other tokens\n",
    "        dec_input_ids = model_batch['dec_input_ids'][..., :1]\n",
    "        dec_attention_mask = model_batch['dec_attention_mask'][..., :1, :1]\n",
    "        dec_position_ids = model_batch['dec_position_ids'][..., :1]\n",
    "        # we use past_key_values, so only the current token mask is needed\n",
    "        cross_attention_mask = model_batch['cross_attention_mask'][..., :1, :]\n",
    "\n",
    "        unfinished_sents = enc_input_ids.new(enc_input_ids.size(0)).fill_(1)\n",
    "        output_ids = enc_input_ids.new_zeros([enc_input_ids.size(0), 0])\n",
    "        output_probs = torch.zeros(batch_size, 1).to(device)\n",
    "        prob_idx = torch.arange(batch_size)\n",
    "        past_key_values = None\n",
    "        \n",
    "        gen_len = 0\n",
    "        while gen_len < target_length:\n",
    "\n",
    "            dec_outputs = model(\n",
    "                dec_input_ids=dec_input_ids,\n",
    "                dec_position_ids=dec_position_ids,\n",
    "                dec_attention_mask=dec_attention_mask,\n",
    "                cross_attention_mask=cross_attention_mask,\n",
    "                enc_hidden_states=enc_hidden_states,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            lm_logits = dec_outputs['lm_logits']\n",
    "            past_key_values = dec_outputs['past_key_values']\n",
    "\n",
    "            logits = lm_logits[:, -1, :] / temperature\n",
    "\n",
    "            prev_output_tokens = torch.cat([enc_input_ids, output_ids], dim=-1)\n",
    "\n",
    "            # repetition_penalty\n",
    "            if repetition_penalty != 1.0:\n",
    "                for i in range(logits.size(0)):\n",
    "                    for previous_token in set(prev_output_tokens[i].tolist()):\n",
    "                        # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
    "                        if logits[i, previous_token] < 0:\n",
    "                            logits[i, previous_token] *= repetition_penalty\n",
    "                        else:\n",
    "                            logits[i, previous_token] /= repetition_penalty\n",
    "\n",
    "            # no_repeat_ngram_size\n",
    "            if no_repeat_ngram_size > 0:\n",
    "                banned_batch_tokens = calc_banned_ngram_tokens(\n",
    "                    output_ids, logits.size(0), no_repeat_ngram_size, gen_len, logits.size(1)\n",
    "                )\n",
    "                for i, banned_tokens in enumerate(banned_batch_tokens):\n",
    "                    logits[i, banned_tokens] = -1e5\n",
    "\n",
    "            logits = top_k_logits(logits, top_k=top_k, top_p=top_p, remove_unk=True)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            next_prob = probs[prob_idx, next_token]\n",
    "            tokens_to_add = next_token * unfinished_sents + tokenizer.sep_id * (1 - unfinished_sents)\n",
    "            probs_to_add = next_prob * unfinished_sents\n",
    "            output_probs = torch.cat([output_probs, probs_to_add.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            dec_input_ids = tokens_to_add.unsqueeze(-1)\n",
    "            output_ids = torch.cat([output_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "            dec_position_ids = dec_position_ids[:, -1:] + 1\n",
    "            # let the current token attend to all previous tokens\n",
    "            dec_attention_mask = torch.cat([dec_attention_mask, dec_attention_mask[:, :, :, -1:]], dim=-1)\n",
    "\n",
    "            if tokenizer.sep_id == tokens_to_add.detach().cpu().numpy().flatten()[0]:\n",
    "                return tokenizer.decode(\n",
    "                    output_ids.detach().cpu().numpy()[0][:-1]\n",
    "                )\n",
    "            \n",
    "            gen_len += 1\n",
    "        \n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    history = []\n",
    "    max_history = 5\n",
    "    while True:\n",
    "        text = input('>>>')\n",
    "        if text == 'quit' or len(text.strip()) <= 0:\n",
    "            break\n",
    "        history.append(text)\n",
    "        history = history[-max_history:]\n",
    "        out = generate_samples(model, tokenizer, history)\n",
    "        print(out)\n",
    "        history.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>你好\n",
      "您好,有什么问题我可以帮您处理或解决呢?#e-s[数字x]\n",
      ">>>\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>你喜欢听相声吗？\n",
      "我很少看电视,除了看电视的时候就没别的兴趣了。\n",
      ">>>你喜欢郭德纲吗\n",
      "不喜欢他的歌啊。\n",
      ">>>郭德纲是干嘛的？\n",
      "什么叫做相声演员,怎么会没有人知道呢?\n",
      ">>>那你听过他的作品吗\n",
      "当然了,不过我很少看相声,要么就是因为郭老师的一些事情才关注他的,你可以去搜一下他以前的节目,或者看看有哪些好玩的。\n",
      ">>>那你知道他的徒弟吗\n",
      "不清楚,但你要问他师傅就一定不是他师傅吧?\n",
      ">>>\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>今天好倒霉啊\n",
      "我下午打球都痛到没知觉了,现在才睡醒\n",
      ">>>我说我好倒霉啊\n",
      "你这也太衰了吧。。。\n",
      ">>>你不问问吗\n",
      "真的很烦恼,我的天\n",
      ">>>你烦恼什么\n",
      "我还没睡,就是突然很想哭。。\n",
      ">>>为什么想哭\n",
      "因为没有人陪。。\n",
      ">>>我陪着你啊\n",
      "好吧,,晚安~\n",
      ">>>\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>我想要去土耳其\n",
      "你不是说就快了嘛\n",
      ">>>嗯，下个月去\n",
      "哦哦!那还挺好的。就是太冷了。\n",
      ">>>温度还好吧\n",
      "也还好啊,冷是因为有暖气\n",
      ">>>\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
