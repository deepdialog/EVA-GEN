{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5EncoderModel, T5Model\n",
    "from transformers import T5Config\n",
    "\n",
    "from tokenization_enc_dec import EncDecTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EncDecTokenizer('./EVA/src/bpe_dialog_new/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp_rank_00_model_states.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../eva-ckpt/222500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../eva-ckpt/222500/mp_rank_00_model_states.pt', map_location='cpu')['module']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeds.weight torch.Size([30000, 2048])\n",
      "lm_head.weight torch.Size([30000, 2048])\n",
      "encoder.word_embeds.weight torch.Size([30000, 2048])\n",
      "encoder.final_layernorm.weight torch.Size([2048])\n",
      "encoder.blocks.0.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 32])\n",
      "encoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.0.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.0.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.1.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.1.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.1.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.2.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.2.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.2.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.3.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.3.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.3.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.4.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.4.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.4.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.5.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.5.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.5.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.6.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.6.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.6.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.7.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.7.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.7.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.8.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.8.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.8.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.9.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.9.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.9.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.10.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.10.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.10.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.11.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.11.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.11.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.12.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.12.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.12.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.13.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.13.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.13.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.14.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.14.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.14.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.15.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.15.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.15.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.16.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.16.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.16.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.17.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.17.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.17.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.18.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.18.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.18.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.19.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.19.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.19.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.20.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.20.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.20.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.21.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.21.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.21.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.22.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.22.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.22.ff.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.23.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "encoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "encoder.blocks.23.self_attn.layer_norm.weight torch.Size([2048])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.blocks.23.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.word_embeds.weight torch.Size([30000, 2048])\n",
      "decoder.final_layernorm.weight torch.Size([2048])\n",
      "decoder.blocks.0.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 32])\n",
      "decoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.0.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.0.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.0.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.0.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.1.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.1.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.1.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.1.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.1.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.2.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.2.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.2.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.2.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.2.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.3.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.3.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.3.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.3.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.3.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.4.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.4.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.4.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.4.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.4.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.5.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.5.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.5.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.5.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.5.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.6.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.6.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.6.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.6.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.6.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.7.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.7.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.7.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.7.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.7.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.8.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.8.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.8.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.8.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.8.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.9.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.9.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.9.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.9.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.9.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.10.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.10.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.10.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.10.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.10.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.11.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.11.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.11.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.11.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.11.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.12.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.12.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.12.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.12.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.12.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.13.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.13.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.13.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.13.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.13.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.14.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.14.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.14.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.14.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.14.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.15.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.15.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.15.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.15.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.15.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.16.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.16.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.16.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.16.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.16.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.17.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.17.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.17.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.17.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.17.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.18.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.18.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.18.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.18.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.18.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.19.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.19.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.19.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.19.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.19.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.20.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.20.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.20.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.20.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.20.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.21.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.21.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.21.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.21.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.21.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.22.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.22.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.22.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.22.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.22.ff.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.23.self_attn.self_attn.project.weight torch.Size([6144, 2048])\n",
      "decoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.23.self_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_q.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_kv.weight torch.Size([4096, 2048])\n",
      "decoder.blocks.23.cross_attn.cross_attn.dense.weight torch.Size([2048, 2048])\n",
      "decoder.blocks.23.cross_attn.layer_norm.weight torch.Size([2048])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([5120, 2048])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([2048, 5120])\n",
      "decoder.blocks.23.ff.layer_norm.weight torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# 注意encoder和decoder的blocks.0和其他的1,2,3...23不一样，0多了relative_attention_bias\n",
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(name):\n",
    "    return state_dict[name].numpy()\n",
    "\n",
    "encoder_names0 = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names0 = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "encoder_names = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "def get_block_weight(n, t='encoder', dim=2048):\n",
    "    weights = []\n",
    "    for k, v in state_dict.items():\n",
    "        if t in k and f'blocks.{n}.' in k:\n",
    "            # pytorch和tensorflow版本的weights是矩阵转置的\n",
    "            w = v.numpy()\n",
    "            if 'self_attn.project' in k:\n",
    "                w0, w1, w2 = w[:dim, :], w[dim:dim*2, :], w[dim*2:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "#                 w2 = np.transpose(w2)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "                weights.append((k, w2))\n",
    "            elif 'cross_attn.project_q' in k:\n",
    "#                 w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "            elif 'cross_attn.project_kv' in k:\n",
    "                w0, w1 = w[:dim, :], w[dim:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "            else:\n",
    "#                 if 'dense' in k:\n",
    "#                     w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "    if 'relative_attention_bias' in weights[3][0]:\n",
    "        weights[3], weights[4] = weights[4], weights[3]\n",
    "    weights = [x[1] for x in weights]\n",
    "    if 'encoder' == t:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(encoder_names0 if n == 0 else encoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    else:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(decoder_names0 if n == 0 else decoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config(\n",
    "    vocab_size=30000,\n",
    "    # n_positions=self.n_positions,\n",
    "    d_model=2048,\n",
    "    d_ff=5120,\n",
    "    d_kv=2048 // 32,\n",
    "    num_layers=24,\n",
    "    num_heads=32,\n",
    "    relative_attention_num_buckets=32,\n",
    "    dropout_rate=0.0,\n",
    "    initializer_factor=1.0,\n",
    "    eos_token_id=tokenizer.eod_id,\n",
    "    bos_token_id=tokenizer.pad_id,\n",
    "    pad_token_id=tokenizer.pad_id,\n",
    "    decoder_start_token_id=tokenizer.pad_id,\n",
    "    feed_forward_proj='gated-gelu',\n",
    "    tie_word_embeddings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5EncoderModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=torch.LongTensor([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight torch.Size([30000, 2048])\n",
      "encoder.embed_tokens.weight torch.Size([30000, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 32])\n",
      "encoder.block.0.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.0.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.1.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.1.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.2.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.2.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.3.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.3.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.4.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.4.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.5.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.5.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.6.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.6.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.7.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.7.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.8.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.8.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.9.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.9.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.10.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.10.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.11.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.11.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.12.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.12.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.12.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.13.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.13.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.13.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.14.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.14.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.14.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.15.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.15.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.15.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.16.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.16.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.16.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.17.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.17.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.17.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.18.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.18.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.18.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.19.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.19.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.19.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.20.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.20.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.20.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.21.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.21.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.21.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.22.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.22.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.22.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.23.layer.0.SelfAttention.q.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.SelfAttention.k.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.SelfAttention.v.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.SelfAttention.o.weight torch.Size([2048, 2048])\n",
      "encoder.block.23.layer.0.layer_norm.weight torch.Size([2048])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0.weight torch.Size([5120, 2048])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1.weight torch.Size([5120, 2048])\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight torch.Size([2048, 5120])\n",
      "encoder.block.23.layer.1.layer_norm.weight torch.Size([2048])\n",
      "encoder.final_layer_norm.weight torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# transformers的T5是把QKV分开的\n",
    "for k, v in model.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.0.layer.0.SelfAttention.q.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight (2048, 2048)\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 32)\n",
      "encoder.block.0.layer.0.layer_norm.weight (2048,)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight (5120, 2048)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight (5120, 2048)\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight (2048, 5120)\n",
      "encoder.block.0.layer.1.layer_norm.weight (2048,)\n"
     ]
    }
   ],
   "source": [
    "for x in get_block_weight(0, t='encoder').items():\n",
    "    print(x[0], x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.1.layer.0.SelfAttention.q.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight (2048, 2048)\n",
      "encoder.block.1.layer.0.layer_norm.weight (2048,)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight (5120, 2048)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight (5120, 2048)\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight (2048, 5120)\n",
      "encoder.block.1.layer.1.layer_norm.weight (2048,)\n"
     ]
    }
   ],
   "source": [
    "for x in get_block_weight(1, t='encoder').items():\n",
    "    print(x[0], x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new_weights = OrderedDict()\n",
    "model_new_weights['shared.weight'] = get_weight('word_embeds.weight')\n",
    "model_new_weights['encoder.embed_tokens.weight'] = get_weight('encoder.word_embeds.weight')\n",
    "for i in range(24):\n",
    "    for k, v in get_block_weight(i, t='encoder').items():\n",
    "        model_new_weights[k] = v\n",
    "\n",
    "model_new_weights['encoder.final_layer_norm.weight'] = get_weight('encoder.final_layernorm.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict({k: torch.from_numpy(v) for k, v in model_new_weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.429 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([tokenizer.encode('你好啊')])\n",
    "out = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1067, -0.5003,  1.5315,  ...,  0.9334, -0.2049,  0.6831],\n",
       "         [-0.2470,  0.2903,  1.9027,  ...,  0.8313, -0.0507,  1.5255]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./torch_eva_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6G\ttorch_eva_encoder\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh torch_eva_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
