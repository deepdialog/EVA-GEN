{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_all_providers\n",
    "from tokenization_enc_dec import EncDecTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_for_provider(model_path: str, provider: str= 'CPUExecutionProvider') -> InferenceSession:\n",
    "    assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
    "    # Few properties that might have an impact on performances (provided by MS)\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 100\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    # Load the model as a graph and prepare the CPU backend\n",
    "    session = InferenceSession(model_path, options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_model_for_provider('./onnx_eva_q/encoder.onnx')\n",
    "decoder = create_model_for_provider('./onnx_eva_q/decoder.onnx')\n",
    "lm = create_model_for_provider('./onnx_eva_q/lm.onnx')\n",
    "tokenizer = EncDecTokenizer('./EVA/src/bpe_dialog_new/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ban = [\n",
    "    5641,  # 京东\n",
    "    4087,  # 客服\n",
    "    2184,  # #\n",
    "    175,   # [\n",
    "    12539, # 小妹\n",
    "    724,   # 客户\n",
    "    1468,  # 商品\n",
    "    6111,  # vi\n",
    "    6454,  # 订单\n",
    "    3748,  # 商家\n",
    "    1548,  # 咨询\n",
    "    6391,  # 发票\n",
    "    681,   # 单\n",
    "    5942,  # 上门\n",
    "    4129,  # 售后\n",
    "    6756,  # 卖家\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk(\n",
    "    s=['你好'],\n",
    "    num_returns=1,\n",
    "    top_k=50,\n",
    "    top_p=1.0,\n",
    "    temperature=1.0,\n",
    "    max_len=64,\n",
    "    ban=default_ban\n",
    "):\n",
    "    input_ids = []\n",
    "    for ss in s:\n",
    "        input_ids += tokenizer.encode(ss) + [tokenizer.sep_id]\n",
    "    input_ids += [tokenizer.get_sentinel_id(0)]\n",
    "    input_ids = np.array([input_ids])\n",
    "    mask = np.ones_like(input_ids)\n",
    "    encoder_last_hidden_state = encoder.run(['last_hidden_state'], {\n",
    "        \"input_ids\": input_ids, \"attention_mask\": mask\n",
    "    })[0]\n",
    "    encoder_last_hidden_state = np.repeat(encoder_last_hidden_state, num_returns, axis=0)\n",
    "    mask = np.repeat(mask, num_returns, axis=0)\n",
    "    decoder_input_ids = np.repeat(np.array([[tokenizer.get_sentinel_id(0)]]), num_returns, axis=0)\n",
    "    decoder_mask = np.ones_like(decoder_input_ids)\n",
    "    choice_inds = list(range(top_k))\n",
    "    all_probs = None\n",
    "    finished = []\n",
    "    for i in range(max_len):\n",
    "        decoder_last_hidden_state = decoder.run(['last_hidden_state'], {\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"decoder_attention_mask\": decoder_mask,\n",
    "            'encoder_hidden_states': encoder_last_hidden_state,\n",
    "            'encoder_attention_mask': mask,\n",
    "        })[0]\n",
    "        logits = lm.run(['logits'], { 'decoder_hidden_states': decoder_last_hidden_state, })[0]\n",
    "\n",
    "        for x in ban:\n",
    "            logits[:, -1, x] = -9999\n",
    "\n",
    "        logits = logits / temperature\n",
    "        scores = softmax(logits[:, -1, :])\n",
    "        next_probs = np.sort(scores)[:, ::-1]\n",
    "        if top_p > 0.0 and top_p < 1.0:\n",
    "            next_probs = next_probs[:, :int(next_probs.shape[1] * (1 - top_p))]\n",
    "        if top_k > 0 and top_k < next_probs.shape[1]:\n",
    "            next_probs = next_probs[:, :top_k]\n",
    "        next_probs_1 = next_probs / next_probs.sum(axis=1).reshape((-1, 1))\n",
    "\n",
    "        next_tokens = np.argsort(scores)[:, ::-1]\n",
    "        if top_p > 0.0 and top_p < 1.0:\n",
    "            next_tokens = next_tokens[:, :int(next_tokens.shape[1] * (1 - top_p))]\n",
    "        if top_k > 0 and top_k < next_probs.shape[1]:\n",
    "            next_tokens = next_tokens[:, :top_k]\n",
    "\n",
    "        inds = np.random.choice(\n",
    "            choice_inds,\n",
    "            num_returns,\n",
    "            p=next_probs_1[0],\n",
    "            replace=True\n",
    "        )\n",
    "        next_tokens = np.array([\n",
    "            x[i]\n",
    "            for x, i in zip(next_tokens, inds)\n",
    "        ])\n",
    "        next_probs = np.array([\n",
    "            x[i]\n",
    "            for x, i in zip(next_probs, inds)\n",
    "        ])\n",
    "        if all_probs is None:\n",
    "            all_probs = np.log(next_probs).reshape(-1, 1)\n",
    "        else:\n",
    "            all_probs = np.concatenate([\n",
    "                all_probs,\n",
    "                np.log(next_probs).reshape(-1, 1)\n",
    "            ], axis=1)\n",
    "        decoder_input_ids = np.concatenate([\n",
    "            decoder_input_ids,\n",
    "            np.array(next_tokens).reshape((-1, 1))\n",
    "        ], axis=1)\n",
    "        for i in reversed(range(decoder_input_ids.shape[0])):\n",
    "            if tokenizer.sep_id in decoder_input_ids[i]:\n",
    "                finished.append((decoder_input_ids[i], all_probs[i]))\n",
    "                decoder_input_ids = np.concatenate([\n",
    "                    decoder_input_ids[:i],\n",
    "                    decoder_input_ids[i + 1:]\n",
    "                ])\n",
    "                all_probs = np.concatenate([\n",
    "                    all_probs[:i],\n",
    "                    all_probs[i + 1:]\n",
    "                ])\n",
    "                encoder_last_hidden_state = np.concatenate([\n",
    "                    encoder_last_hidden_state[:i],\n",
    "                    encoder_last_hidden_state[i + 1:]\n",
    "                ])\n",
    "                mask = np.concatenate([\n",
    "                    mask[:i],\n",
    "                    mask[i + 1:]\n",
    "                ])\n",
    "        if len(decoder_input_ids) == 0:\n",
    "            break\n",
    "        decoder_mask = np.ones_like(decoder_input_ids)\n",
    "    rets = [\n",
    "        tokenizer.decode(x[0])\n",
    "        for x in finished\n",
    "    ]\n",
    "    scores = [\n",
    "        np.mean(x[1])\n",
    "        for x in finished\n",
    "    ]\n",
    "    return rets, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 35s, sys: 14.7 s, total: 6min 49s\n",
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "finished = talk(max_len=64, num_returns=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s_0>请问您是要修改什么信息呢<sep>',\n",
       " '<s_0>请问您是要修改什么信息呢<sep>',\n",
       " '<s_0>有什么可以帮到您的吗?<sep>',\n",
       " '<s_0>您好,请问有什么可以帮您?<sep>',\n",
       " '<s_0>您好,请问有什么可以帮您?<sep>',\n",
       " '<s_0>您好,请问有什么可以帮您?<sep>',\n",
       " '<s_0>您好,请问有什么可以帮您?<sep>',\n",
       " '<s_0>请问有什么问题我可以帮您处理或解决呢?<sep>',\n",
       " '<s_0>请问有什么问题我可以帮您处理或解决呢?你好<sep>',\n",
       " '<s_0>您好,请您提供一下您的姓名和联系方式,我们会在今天下午(周日)下午(周六)为您回电,请您注意接听电话哈<sep>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7.2126923,\n",
       " -7.2126923,\n",
       " -5.9243674,\n",
       " -4.9900384,\n",
       " -4.9900384,\n",
       " -4.9900384,\n",
       " -4.9900384,\n",
       " -2.449787,\n",
       " -3.0436127,\n",
       " -2.7436812]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s_0>请问有什么问题我可以帮您处理或解决呢?<sep>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished[0][np.argmax(finished[1])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
